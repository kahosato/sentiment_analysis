%- python 2.7
%- tokenisation
%- 10-cross validation
%	- randomise
\subsection{Tokenisation}
% - split in to sentences
% 	- \n
% - decapitalise the first letter
% - split with white space
% - for each token, apply the following rules conseqtively
%split_tokens = Tokeniser.__split_hyphen(split_tokens)
%        split_tokens = Tokeniser.__split_slash(split_tokens)
%        split_tokens = Tokeniser.__split_comma(split_tokens)
%        split_tokens = Tokeniser.__split_left_bracket(split_tokens)
%        split_tokens = Tokeniser.__split_right_bracket(split_tokens)
%        split_tokens = Tokeniser.__split_ll(split_tokens)
%        split_tokens = Tokeniser.__split_nt(split_tokens)
%        split_tokens = Tokeniser.__split_d(split_tokens)
%        split_tokens = Tokeniser.__split_s(split_tokens)
%        split_tokens = Tokeniser.__split_ve(split_tokens)
%        split_tokens = Tokeniser.__split_last_period(split_tokens)
\subsection{Cross Validation}
% put all the documents in one array, shuffle them and split into n
% non deterministic
