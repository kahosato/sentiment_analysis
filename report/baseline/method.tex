%- python 2.7
%- tokenisation
%- 10-cross validation
%	- randomise
This section describes implementation choices made while replicating the methods described in Section \ref{sec:background}.
\subsection{Tokenisation}
In this section, the process of converting a input documents into a stream of tokens is described.
A document is first split into sentences, which get tokenised separately.
The first letter of a sentence is capitalised, and it gets split at the white spaces to give the initial list of tokens.
Then, the following rules are applied sequentially to each token in the list.
\begin{itemize}
\item Split at hyphens only if all the letters used are alphabets.
(e.g. ``long-term'' $\rightarrow$ [``long'', ``-'', ``term''], ``3-G21'' $\rightarrow$ [``3-G21''])
\item Split at slash, comma and brackets.
\item Split ``'ll'' and add a token ``will'' instead.
(e.g. ``you'll'' $\rightarrow$ [``you'', ``will''])
\item Split ``n't'' and add a token ``not'' instead.
(e.g. ``don't'' $\rightarrow$ [``do'', ``not''])
\item Split ``'d'' and ``'s''.
(e.g. ``you'd'' $\rightarrow$ [``you'', ``'d''])
\item Split ``'ve'' and add a token ``have'' instead.
(e.g. ``you've'' $\rightarrow$ [``you'', ``have''])
\item Split trailing question marks, exclamation marks, periods, colons and semicolons.
(e.g. ``done?!'' $\rightarrow$ [``done'', ``??''])
\end{itemize}
Note that all the punctuations are kept in the resulting tokens.
\subsection{Cross Validation}
% put all the documents in one array, shuffle them and split into n
% non deterministic

\subsection{Symbolic Approach}
\subsubsection{Sentiment Score}
\subsection{Naive Bayes Classifier}
\subsection{features}
% no stemming, no stoplist
%unigram
\subsection{Smoothing}
